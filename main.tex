\documentclass{article}

% NeurIPS style
\usepackage[final]{neurips_2024}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds,calc}
\usepackage{fancybox}
\usepackage{framed}

% Custom Commands
\newcommand{\torchsla}{\texttt{torch-sla}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\cO}{\mathcal{O}}

% Algorithm counter
\newcounter{algocounter}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    stringstyle=\color{red!70!black},
    commentstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=3pt,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!5},
    xleftmargin=2em,
    framexleftmargin=1.5em
}

% Title
\title{%
    \textbf{torch-sla}: Differentiable Sparse Linear Algebra with \\
    Adjoint Solvers and Sparse Tensor Parallelism for PyTorch
}

\author{
    Mingyuan Chi \\
    \texttt{walker.chi.000@gmail.com} \\
    \url{https://github.com/walkerchi/torch-sla}
}

\begin{document}

\maketitle

% ============================================================================
% Abstract
% ============================================================================
\begin{abstract}
We present \torchsla{}, an open-source PyTorch library for differentiable sparse linear algebra. The library addresses two fundamental challenges: (1) \textbf{Adjoint-based differentiation} for linear and nonlinear sparse solvers, achieving $\cO(1)$ memory complexity for computational graphs regardless of solver iterations; and (2) \textbf{Sparse Tensor Parallel} computing via domain decomposition with halo exchange, enabling distributed sparse operations across multiple GPUs. \torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and scales to 169 million degrees of freedom. Benchmarks demonstrate $12\times$ speedup on multi-GPU configurations and correct gradient computation verified against finite differences.
\end{abstract}

% ============================================================================
% Introduction
% ============================================================================
\section{Introduction}
\label{sec:intro}

Sparse linear systems $\bA\bx = \bb$ are fundamental to scientific computing. They arise in finite element analysis~\citep{hughes2012finite}, graph neural networks~\citep{kipf2017semi,velivckovic2018graph}, physics-informed machine learning~\citep{raissi2019physics,lu2021learning}, and computational fluid dynamics~\citep{jasak2007openfoam,kochkov2021machine}. With the rise of differentiable programming and neural operators~\citep{li2020fourier,brandstetter2022message}, there is growing demand for sparse solvers that integrate with automatic differentiation frameworks like PyTorch~\citep{paszke2019pytorch}.

\textbf{The challenge of differentiating through solvers.} Consider a loss function $\mathcal{L}(\bx)$ where $\bx = \bA^{-1}\bb$ is obtained by solving a linear system. To train neural networks that interact with such solvers, we need gradients $\partial\mathcal{L}/\partial\bA$ and $\partial\mathcal{L}/\partial\bb$. A naive approach---differentiating through each iteration of an iterative solver---creates $\cO(k)$ nodes in the computational graph, where $k$ may be thousands of iterations. This leads to memory explosion and slow backward passes.

\textbf{The challenge of scale.} Industrial problems often exceed single-GPU memory. A 3D finite element mesh with 10 million nodes produces a sparse matrix requiring tens of gigabytes. Distributed computing is essential, but sparse matrix operations require careful communication patterns (halo exchange) that are non-trivial to implement correctly with gradient support.

This paper introduces \torchsla{}, addressing both challenges with two key innovations:
\begin{itemize}
    \item \textbf{Adjoint-based differentiation} (\S\ref{sec:adjoint}): We implement the adjoint method for linear solves, eigenvalue problems, and nonlinear systems. This achieves $\cO(1)$ computational graph nodes and $\cO(\text{nnz})$ memory, independent of solver iterations.
    
    \item \textbf{Sparse tensor parallelism} (\S\ref{sec:distributed}): We implement domain decomposition with automatic halo exchange following industrial CFD/FEM practices. This enables multi-GPU computing with gradient support.
\end{itemize}

% ============================================================================
% Related Work
% ============================================================================
\section{Related Work}
\label{sec:related}

\textbf{Differentiable linear algebra.} JAX~\citep{jax2018github} provides differentiable sparse CG, but naive differentiation through iterations creates $\cO(k)$ graph nodes. \citet{blondel2022efficient} formalize implicit differentiation for optimization layers. OptNet~\citep{amos2017optnet} and cvxpylayers~\citep{agrawal2019differentiable} handle convex optimization. Deep equilibrium models~\citep{bai2019deep} and Jacobian-free backpropagation~\citep{fung2022jfb} address implicit layers but focus on fixed-point iterations rather than sparse linear systems.

\textbf{Sparse solvers and GPU acceleration.} SciPy~\citep{scipy2020} provides SuperLU/UMFPACK for CPU. For GPU, NVIDIA cuDSS~\citep{nvidia2024cudss} offers direct solvers, while AmgX~\citep{naumov2015amgx} provides algebraic multigrid with excellent scalability. Recent surveys~\citep{li2024sparse} cover sparse matrix computations on modern GPUs. None of these natively support PyTorch autograd.

\textbf{Distributed sparse computing.} PETSc~\citep{balay2019petsc}, Trilinos~\citep{heroux2005overview}, and hypre~\citep{falgout2002hypre} implement distributed sparse linear algebra with sophisticated preconditioners. OpenFOAM~\citep{jasak2007openfoam} uses domain decomposition for CFD. We bring these industrial patterns to PyTorch with automatic differentiation.

\textbf{Learned solvers and preconditioners.} Recent work explores learning components of iterative solvers: learned multigrid prolongation~\citep{greenfeld2019learning}, GNN-based AMG~\citep{luz2020learning}, and learned interface conditions for domain decomposition~\citep{taghibakhshi2024learning}. These complement our library by providing learnable components that can be trained end-to-end.

% ============================================================================
% Methodology
% ============================================================================
\section{Methodology}
\label{sec:method}

We first introduce the background on sparse linear systems (\S\ref{sec:background}), then present our adjoint-based differentiation approach (\S\ref{sec:adjoint}), and finally describe our distributed computing implementation (\S\ref{sec:distributed}).

% ----------------------------------------------------------------------------
\subsection{Preliminaries: Sparse Linear Systems}
\label{sec:background}

A sparse linear system $\bA\bx = \bb$ has a matrix $\bA \in \R^{n \times n}$ with $\text{nnz} \ll n^2$ non-zero entries. We store matrices in coordinate (COO) format: three arrays for row indices, column indices, and values.

\textbf{Direct solvers} (LU, Cholesky factorization) compute exact solutions but require $\cO(n^{1.5})$ memory for 2D problems due to fill-in---new non-zeros created during factorization~\citep{george1973nested}. For a 2D Poisson problem with $n$ unknowns, the factored matrix has $\cO(n \log n)$ non-zeros instead of the original $\cO(n)$.

\textbf{Iterative solvers} (Conjugate Gradient, BiCGStab, GMRES) maintain $\cO(\text{nnz})$ memory by never forming the factorization. However, they require $\cO(\sqrt{\kappa})$ iterations for CG where $\kappa$ is the condition number~\citep{saad2003iterative}. Each iteration performs one sparse matrix-vector multiplication (SpMV) costing $\cO(\text{nnz})$.

\textbf{The differentiation problem.} Given loss $\mathcal{L}(\bx)$ where $\bx = \bA^{-1}\bb$, we need gradients with respect to both $\bb$ and the non-zero values of $\bA$. If we differentiate through $k$ iterations of CG, the computational graph has $\cO(k)$ nodes, each storing intermediate vectors. For $k = 1000$ iterations and $n = 10^6$, this requires $\sim$80 GB just for the graph. Our adjoint approach reduces this to $\cO(1)$ nodes.

% ----------------------------------------------------------------------------
\subsection{Adjoint-Based Differentiation}
\label{sec:adjoint}

The adjoint method computes gradients through implicit functions without storing intermediate solver states. We present it for linear systems, eigenvalue problems, and nonlinear systems.

\subsubsection{Linear Systems: The Core Idea}

Consider $\bx = \bA^{-1}\bb$. Rather than differentiating through the solver iterations, we use the \emph{implicit function theorem}. The solution satisfies $\bA\bx - \bb = \mathbf{0}$. Differentiating implicitly:
\begin{equation}
    d\bA \cdot \bx + \bA \cdot d\bx = d\bb \implies d\bx = \bA^{-1}(d\bb - d\bA \cdot \bx)
\end{equation}

For a scalar loss $\mathcal{L}(\bx)$, the chain rule gives:
\begin{equation}
    d\mathcal{L} = \left(\frac{\partial \mathcal{L}}{\partial \bx}\right)^\top d\bx = \left(\frac{\partial \mathcal{L}}{\partial \bx}\right)^\top \bA^{-1}(d\bb - d\bA \cdot \bx)
\end{equation}

Define the \textbf{adjoint variable} $\blambda = \bA^{-\top}\frac{\partial \mathcal{L}}{\partial \bx}$, solved via $\bA^\top \blambda = \frac{\partial \mathcal{L}}{\partial \bx}$. Then the gradients are:
\begin{equation}
    \boxed{\frac{\partial \mathcal{L}}{\partial \bb} = \blambda, \quad
    \frac{\partial \mathcal{L}}{\partial \bA_{ij}} = -\blambda_i \cdot \bx_j}
    \label{eq:adjoint_linear}
\end{equation}

\textbf{Complexity analysis.} Table~\ref{tab:complexity} compares naive differentiation (through iterations) with our adjoint approach.

\begin{table}[t]
\centering
\caption{Complexity comparison: naive vs. adjoint differentiation through iterative solver with $k$ iterations, $n$ unknowns, and nnz non-zeros.}
\label{tab:complexity}
\vspace{0.5em}
\small
\begin{tabular}{lcc}
\toprule
& \textbf{Naive (through iterations)} & \textbf{Adjoint (ours)} \\
\midrule
Computational graph nodes & $\cO(k)$ & $\cO(1)$ \\
Memory for graph & $\cO(k \cdot n)$ & $\cO(n + \text{nnz})$ \\
Backward pass time & $\cO(k \cdot \text{nnz})$ & $\cO(T_{\text{solve}} + \text{nnz})$ \\
\bottomrule
\end{tabular}
\end{table}

The forward solve computes $\bx$ in time $T_{\text{solve}}$. The backward pass:
\begin{enumerate}
    \item Solves $\bA^\top\blambda = \frac{\partial\mathcal{L}}{\partial\bx}$: $\cO(T_{\text{solve}})$ time
    \item Computes $\frac{\partial\mathcal{L}}{\partial\bA_{ij}} = -\blambda_i \bx_j$ for each non-zero: $\cO(\text{nnz})$ time
\end{enumerate}

\textbf{Algorithm.} The complete procedure is shown below:

\begin{framed}
\refstepcounter{algocounter}
\noindent\textbf{Algorithm \thealgocounter: Adjoint Linear Solve}
\vspace{0.3em}
\hrule
\vspace{0.5em}
\noindent\textbf{Input:} Sparse matrix $\bA$ (values, rows, cols), RHS $\bb$\\
\textbf{Output:} Solution $\bx$, gradients in backward pass

\vspace{0.3em}
\textbf{Forward pass:}
\begin{enumerate}
    \item $\bx \gets \text{solve}(\bA, \bb)$ \hfill \textit{// Any solver: CG, LU, etc.}
    \item Store $(\bA, \bx)$ for backward
\end{enumerate}

\textbf{Backward pass:} (given $\partial\mathcal{L}/\partial\bx$)
\begin{enumerate}
    \item $\blambda \gets \text{solve}(\bA^\top, \partial\mathcal{L}/\partial\bx)$ \hfill \textit{// One adjoint solve}
    \item $\partial\mathcal{L}/\partial\bb \gets \blambda$
    \item For each non-zero $(i,j)$: $\partial\mathcal{L}/\partial\bA_{ij} \gets -\blambda_i \cdot \bx_j$ \hfill \textit{// $\cO(\text{nnz})$ total}
\end{enumerate}
\end{framed}

\subsubsection{Eigenvalue Problems}

For symmetric eigenvalue problem $\bA\bv = \lambda\bv$ with normalized eigenvector $\|\bv\|=1$, the gradient of eigenvalue $\lambda$ with respect to matrix entries is remarkably simple~\citep{magnus1985differentiating}:
\begin{equation}
    \frac{\partial \lambda}{\partial \bA_{ij}} = v_i \cdot v_j
    \label{eq:adjoint_eig}
\end{equation}

This is the outer product $\bv\bv^\top$ restricted to the sparsity pattern. For $k$ eigenvalues, the total cost is $\cO(k \cdot \text{nnz})$.

\subsubsection{Nonlinear Systems}

For nonlinear system $\bF(\bu, \btheta) = \mathbf{0}$ with solution $\bu^*(\btheta)$ and loss $\mathcal{L}(\bu^*)$, implicit differentiation gives:
\begin{equation}
    \bJ^\top \blambda = \frac{\partial\mathcal{L}}{\partial\bu}, \quad
    \frac{\partial\mathcal{L}}{\partial\btheta} = -\blambda^\top \frac{\partial\bF}{\partial\btheta}
    \label{eq:adjoint_nonlinear}
\end{equation}
where $\bJ = \frac{\partial\bF}{\partial\bu}$ is the Jacobian at the solution.

\begin{framed}
\refstepcounter{algocounter}
\noindent\textbf{Algorithm \thealgocounter: Newton-Raphson with Adjoint Gradients}
\vspace{0.3em}
\hrule
\vspace{0.5em}
\noindent\textbf{Input:} Residual function $\bF(\bu, \btheta)$, initial guess $\bu_0$\\
\textbf{Output:} Solution $\bu^*$

\vspace{0.3em}
\textbf{Forward (Newton iteration):}
\begin{enumerate}
    \item $\bu \gets \bu_0$
    \item \textbf{while} $\|\bF(\bu, \btheta)\| > \epsilon$ \textbf{do}:
    \begin{enumerate}
        \item $\bJ \gets \partial\bF/\partial\bu$ \hfill \textit{// Jacobian via autograd}
        \item $\Delta\bu \gets \text{solve}(\bJ, -\bF)$
        \item $\bu \gets \bu + \alpha\Delta\bu$ \hfill \textit{// $\alpha$ from line search}
    \end{enumerate}
    \item Store $(\bu^*, \bJ, \btheta)$
\end{enumerate}

\textbf{Backward:} (given $\partial\mathcal{L}/\partial\bu^*$)
\begin{enumerate}
    \item $\blambda \gets \text{solve}(\bJ^\top, \partial\mathcal{L}/\partial\bu^*)$ \hfill \textit{// One adjoint solve}
    \item $\partial\mathcal{L}/\partial\btheta \gets -\blambda^\top \partial\bF/\partial\btheta$ \hfill \textit{// VJP via autograd}
\end{enumerate}
\end{framed}

\textbf{Memory analysis.} Forward: $\cO(n + \text{nnz})$ for solution and Jacobian. Backward: $\cO(n)$ for adjoint variable. Total: $\cO(n + \text{nnz})$, independent of Newton iterations.

% ----------------------------------------------------------------------------
\subsection{Sparse Tensor Parallel Computing}
\label{sec:distributed}

For problems exceeding single-GPU memory, we implement domain decomposition with halo exchange---the standard approach in industrial CFD/FEM codes like OpenFOAM and Ansys Fluent.

\subsubsection{Domain Decomposition}

Given a sparse matrix $\bA$ corresponding to a mesh or graph, we partition the $n$ nodes into $P$ subdomains. Each process $p$ owns nodes $\mathcal{O}_p$ (owned nodes) and maintains copies of neighboring nodes $\mathcal{H}_p$ (halo/ghost nodes) needed for local computation.

\textbf{Partitioning strategies:}
\begin{itemize}
    \item \textbf{METIS}~\citep{karypis1998fast}: Graph partitioning minimizing edge cuts for load balancing
    \item \textbf{RCB}: Recursive Coordinate Bisection using node coordinates  
    \item \textbf{Contiguous}: Simple row-based partitioning (fallback)
\end{itemize}

\subsubsection{Halo Exchange}

The key operation in distributed sparse computing is \textbf{halo exchange}: before each SpMV, processes must exchange boundary values with neighbors. Figure~\ref{fig:halo} illustrates this concept.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
    scale=0.9,
    node/.style={circle, draw, minimum size=7mm, font=\small},
    owned0/.style={node, fill=blue!30},
    owned1/.style={node, fill=red!30},
    halo/.style={node, fill=gray!20, dashed},
    arrow/.style={->, thick, >=stealth}
]
% Process 0 nodes
\node[owned0] (n0) at (0,0) {0};
\node[owned0] (n1) at (1.1,0) {1};
\node[owned0] (n2) at (2.2,0) {2};
\node[halo] (h3) at (3.3,0) {3};

% Process 1 nodes  
\node[halo] (h2) at (4.7,0) {2};
\node[owned1] (n3) at (5.8,0) {3};
\node[owned1] (n4) at (6.9,0) {4};
\node[owned1] (n5) at (8.0,0) {5};

% Labels
\node[above=0.2cm, font=\footnotesize\bfseries] at (1.1,0) {Process 0};
\node[above=0.2cm, font=\footnotesize\bfseries] at (6.35,0) {Process 1};

% Edges
\draw[thick] (n0) -- (n1) -- (n2) -- (h3);
\draw[thick] (h2) -- (n3) -- (n4) -- (n5);

% Halo exchange arrows
\draw[arrow, blue!70, bend left=25] (n2) to node[above, font=\scriptsize] {send} (h2);
\draw[arrow, red!70, bend left=25] (n3) to node[below, font=\scriptsize] {send} (h3);

% Partition boundary
\draw[dashed, thick, gray] (4.0, -0.7) -- (4.0, 0.9);
\node[gray, font=\scriptsize] at (4.0, -0.95) {boundary};

% Legend
\node[owned0, scale=0.6] at (0, -1.5) {};
\node[right, font=\scriptsize] at (0.35, -1.5) {Owned P0};
\node[owned1, scale=0.6] at (2.2, -1.5) {};
\node[right, font=\scriptsize] at (2.55, -1.5) {Owned P1};
\node[halo, scale=0.6] at (4.4, -1.5) {};
\node[right, font=\scriptsize] at (4.75, -1.5) {Halo (ghost)};
\end{tikzpicture}
\caption{Halo exchange in domain decomposition. Each process owns a subset of nodes (solid colored) and maintains halo copies of boundary neighbors (dashed). Before SpMV, processes exchange updated values at partition boundaries via peer-to-peer communication.}
\label{fig:halo}
\end{figure}

\begin{framed}
\refstepcounter{algocounter}
\noindent\textbf{Algorithm \thealgocounter: Distributed SpMV with Halo Exchange}
\vspace{0.3em}
\hrule
\vspace{0.5em}
\noindent\textbf{Input:} Local vector $\bx_{\text{local}}$, neighbor map, local matrix $\bA_{\text{local}}$\\
\textbf{Output:} Result $\by_{\text{owned}}$

\vspace{0.3em}
\textbf{Phase 1: Initiate async communication}
\begin{enumerate}
    \item \textbf{for} each neighbor $q$ \textbf{do}:
    \begin{enumerate}
        \item \texttt{async\_send}(my boundary values to $q$)
        \item \texttt{async\_recv}(halo values from $q$)
    \end{enumerate}
\end{enumerate}

\textbf{Phase 2: Wait for completion}
\begin{enumerate}
    \item \texttt{synchronize\_all}()
\end{enumerate}

\textbf{Phase 3: Local computation}
\begin{enumerate}
    \item $\by_{\text{owned}} \gets \bA_{\text{local}} \cdot \bx_{\text{local}}$ \hfill \textit{// Uses owned + halo}
\end{enumerate}
\end{framed}

\subsubsection{Distributed Conjugate Gradient}

Building on distributed SpMV, we implement distributed CG. Each iteration requires:
\begin{itemize}
    \item One halo exchange (in SpMV)
    \item Two \texttt{all\_reduce} operations for global dot products
\end{itemize}

\begin{framed}
\refstepcounter{algocounter}
\noindent\textbf{Algorithm \thealgocounter: Distributed Conjugate Gradient}
\vspace{0.3em}
\hrule
\vspace{0.5em}
\noindent\textbf{Input:} Distributed $\bA$, local RHS $\bb_{\text{owned}}$, tolerance $\epsilon$\\
\textbf{Output:} Solution $\bx_{\text{owned}}$

\vspace{0.3em}
\begin{enumerate}
    \item $\bx \gets \mathbf{0}$, $\br \gets \bb_{\text{owned}}$, $\bp \gets \br$
    \item $\rho \gets \texttt{all\_reduce}(\br^\top\br, \text{SUM})$ \hfill \textit{// Global dot product}
    \item \textbf{while} $\sqrt{\rho} > \epsilon$ \textbf{do}:
    \begin{enumerate}
        \item $\bA\bp \gets \text{DistSpMV}(\bA, \bp)$ \hfill \textit{// Algorithm 3}
        \item $\alpha \gets \rho / \texttt{all\_reduce}(\bp^\top\bA\bp, \text{SUM})$
        \item $\bx \gets \bx + \alpha\bp$
        \item $\br \gets \br - \alpha\bA\bp$
        \item $\rho_{\text{new}} \gets \texttt{all\_reduce}(\br^\top\br, \text{SUM})$
        \item $\bp \gets \br + (\rho_{\text{new}}/\rho)\bp$
        \item $\rho \gets \rho_{\text{new}}$
    \end{enumerate}
\end{enumerate}
\end{framed}

\textbf{Communication complexity.} Per iteration: $\cO(|\mathcal{H}_p|)$ for halo exchange + $\cO(\log P)$ for all\_reduce. Total per solve: $\cO(k \cdot (|\mathcal{H}_p| + \log P))$ where $k$ is iterations.

\textbf{Gradient support.} Distributed operations compose with adjoint differentiation: the backward pass performs another distributed solve with transposed communication patterns.

% ============================================================================
% Experiments
% ============================================================================
\section{Experiments}
\label{sec:experiments}

We evaluate \torchsla{} on 2D Poisson equations (5-point stencil) using NVIDIA H200 GPUs (140GB HBM3).

\subsection{Single-GPU Scalability}

Table~\ref{tab:benchmark} compares solver backends. Key findings:

\begin{table}[t]
\centering
\caption{Single-GPU benchmark on 2D Poisson, H200, float64.}
\label{tab:benchmark}
\vspace{0.5em}
\small
\begin{tabular}{rrrrrr}
\toprule
\textbf{DOF} & \textbf{SciPy} & \textbf{cuDSS} & \textbf{PyTorch CG} & \textbf{Memory} & \textbf{Residual} \\
\midrule
10K & 24 ms & 128 ms & 20 ms & 36 MB & $10^{-9}$ \\
100K & 29 ms & 630 ms & 43 ms & 76 MB & $10^{-7}$ \\
1M & 19.4 s & 7.3 s & 190 ms & 474 MB & $10^{-7}$ \\
2M & 52.9 s & 15.6 s & 418 ms & 916 MB & $10^{-7}$ \\
16M & OOM & OOM & 7.3 s & 7.1 GB & $10^{-6}$ \\
\textbf{169M} & OOM & OOM & \textbf{224 s} & \textbf{74.8 GB} & $10^{-6}$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Iterative solvers scale}: PyTorch CG reaches 169M DOF with $\cO(n^{1.1})$ time complexity.
    \item \textbf{Direct solvers limited}: cuDSS/SciPy hit OOM at $\sim$2M DOF due to fill-in.
    \item \textbf{Memory efficient}: 443 bytes/DOF for CG (theoretical minimum: 144 bytes/DOF).
\end{itemize}

\subsection{Multi-GPU Performance}

Table~\ref{tab:distributed} shows distributed CG on 4 GPUs with NCCL backend.

\begin{table}[t]
\centering
\caption{Distributed solve on 4$\times$ H200 GPUs.}
\label{tab:distributed}
\vspace{0.5em}
\small
\begin{tabular}{rrrr}
\toprule
\textbf{DOF} & \textbf{Time} & \textbf{Memory/GPU} & \textbf{Speedup vs. CPU} \\
\midrule
10K & 0.18 s & 0.03 GB & 2$\times$ \\
100K & 0.61 s & 0.05 GB & 12$\times$ \\
1M & 2.82 s & 0.27 GB & -- \\
2M & 6.02 s & 0.50 GB & -- \\
\bottomrule
\end{tabular}
\end{table}

With 4 GPUs $\times$ 140GB, theoretical maximum is $\sim$1.3B DOF.

\subsection{Gradient Verification}

We verify gradient correctness against finite differences. All operations achieve relative error $<10^{-5}$, confirming correct adjoint implementation.

% ============================================================================
% Conclusion
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \torchsla{}, a differentiable sparse linear algebra library with two innovations: (1) adjoint-based differentiation achieving $\cO(1)$ graph complexity, and (2) distributed sparse computing with halo exchange. The library scales to 169M DOF on single GPU and provides $12\times$ multi-GPU speedup.

\subsection{Future Work}

\begin{itemize}
    \item \textbf{Learned preconditioners}: GNN-based multigrid~\citep{luz2020learning} and meta-learned preconditioner selection~\citep{chen2022meta} could be integrated for adaptive preconditioning.
    \item \textbf{Mixed precision}: Following FlashAttention~\citep{dao2022flashattention}, we plan FP16/BF16 compute with FP64 accumulation.
    \item \textbf{AmgX integration}: Wrapping NVIDIA AmgX~\citep{naumov2015amgx} as a differentiable backend for algebraic multigrid.
    \item \textbf{Neural operator hybrid}: Combining with FNO~\citep{li2020fourier}, DeepONet~\citep{lu2021learning}, and mesh-based GNNs~\citep{pfaff2021learning} for hybrid neural-classical solvers.
    \item \textbf{Differentiable simulation}: Integration with $\Phi$Flow~\citep{holl2024phiflow} and DiffTaichi~\citep{hu2019difftaichi} for end-to-end differentiable physics.
\end{itemize}

\paragraph{Availability.} MIT license: \url{https://github.com/walkerchi/torch-sla}

% ============================================================================
% References
% ============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

% ============================================================================
% Appendix
% ============================================================================
\appendix
\section{Implementation Details}
\label{app:impl}

\subsection{Backend Selection}

\torchsla{} automatically selects backends based on device and problem size:
\begin{itemize}
    \item CPU, any size: \texttt{scipy} with SuperLU
    \item CUDA, $<$2M DOF: \texttt{cudss} with Cholesky (if SPD) or LU
    \item CUDA, $>$2M DOF: \texttt{pytorch} with CG+Jacobi preconditioner
\end{itemize}

\subsection{API Examples}

\begin{lstlisting}[language=Python,caption={Basic usage with gradient support.}]
import torch
from torch_sla import SparseTensor

# Create sparse matrix (COO format)
A = SparseTensor(val, row, col, shape)
b = torch.randn(n, requires_grad=True)

# Solve with automatic differentiation
x = A.solve(b)  # Forward: any backend
loss = x.sum()
loss.backward()  # Backward: adjoint method
print(b.grad)    # Gradient w.r.t. RHS
\end{lstlisting}

\begin{lstlisting}[language=Python,caption={Distributed multi-GPU solve.}]
import torch.distributed as dist
from torch_sla import DSparseMatrix

dist.init_process_group(backend='nccl')
rank = dist.get_rank()

# Each process loads its partition
A = DSparseMatrix.from_global(
    val, row, col, shape,
    num_partitions=4, my_partition=rank
)

# Distributed CG with halo exchange
x = A.solve(b_local, atol=1e-10)
\end{lstlisting}

\section{Complexity Proofs}
\label{app:proofs}

\textbf{Theorem 1} (Adjoint memory complexity). \textit{The adjoint method for linear solve requires $\cO(n + \text{nnz})$ memory, independent of solver iterations.}

\textit{Proof.} Forward pass stores: solution $\bx \in \R^n$, matrix indices $(\cO(\text{nnz}))$, matrix values $(\cO(\text{nnz}))$. Backward pass computes adjoint $\blambda \in \R^n$ and gradients $\partial\mathcal{L}/\partial\bA \in \R^{\text{nnz}}$. No intermediate solver states are stored. Total: $\cO(n + \text{nnz})$. \qed

\end{document}
